{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDNxPp41gwdj"
   },
   "source": [
    "Importing the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 9613,
     "status": "ok",
     "timestamp": 1706554664066,
     "user": {
      "displayName": "Begüm Nur",
      "userId": "03114851068835061678"
     },
     "user_tz": -180
    },
    "id": "yMn10Tdxg5yC"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.src.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5onWo20Wg7ty"
   },
   "source": [
    "Function For Labeling the Sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1706554664067,
     "user": {
      "displayName": "Begüm Nur",
      "userId": "03114851068835061678"
     },
     "user_tz": -180
    },
    "id": "NfSYzLWrhC8W"
   },
   "outputs": [],
   "source": [
    "def initial_label(sentence):\n",
    "    separated_pattern = re.compile(r'(\\s|^)(de|ki)(\\s|$)', re.IGNORECASE)\n",
    "    not_separated_pattern = re.compile(r'\\w+(de|ki)\\w*', re.IGNORECASE)\n",
    "    if separated_pattern.search(sentence):\n",
    "        return 1  # 'de' or 'ki' is a separate word\n",
    "    elif not_separated_pattern.search(sentence):\n",
    "        return 0  # 'de' or 'ki' is used as a suffix\n",
    "    return None  # Inconclusive or not found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFHO1IulhEJP"
   },
   "source": [
    "Function For Extracting the Sentences That Contains the Suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1706554664067,
     "user": {
      "displayName": "Begüm Nur",
      "userId": "03114851068835061678"
     },
     "user_tz": -180
    },
    "id": "LdbApyLRhMTU"
   },
   "outputs": [],
   "source": [
    "def extract_and_label_sentences(file_path):\n",
    "    print(\"Starting sentence extraction and initial labeling...\")\n",
    "    extracted_sentences = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if ' de ' in line or ' ki ' in line:\n",
    "                sentences = re.split(r'[.!?…]', line)\n",
    "                for sentence in sentences:\n",
    "                    label = initial_label(sentence)\n",
    "                    if label is not None:\n",
    "                        extracted_sentences.append((sentence.strip(), label))\n",
    "    print(f\"Extracted {len(extracted_sentences)} sentences.\")\n",
    "    return extracted_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpJ7OVp9hM8W"
   },
   "source": [
    "Function For Merging the Separated Suffixes With the Previous Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1706554664067,
     "user": {
      "displayName": "Begüm Nur",
      "userId": "03114851068835061678"
     },
     "user_tz": -180
    },
    "id": "KDJL7wI3hYri"
   },
   "outputs": [],
   "source": [
    "def merge_de_ki(sentence):\n",
    "    return sentence.replace(\" de \", \"de \").replace(\" ki \", \"ki \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6L252xWhZNp"
   },
   "source": [
    "Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1706554664067,
     "user": {
      "displayName": "Begüm Nur",
      "userId": "03114851068835061678"
     },
     "user_tz": -180
    },
    "id": "e2KNdKVwhfLG"
   },
   "outputs": [],
   "source": [
    "file_path = 'wiki_00'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTowFIx2hgCh"
   },
   "source": [
    "Extracting and initially labeling sentences from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "executionInfo": {
     "elapsed": 1727,
     "status": "error",
     "timestamp": 1706554665790,
     "user": {
      "displayName": "Begüm Nur",
      "userId": "03114851068835061678"
     },
     "user_tz": -180
    },
    "id": "cdUNc4rrhkdx",
    "outputId": "437ec266-12d1-4439-a6d4-73c6709b593b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting and initially labeling sentences from file...\n",
      "Starting sentence extraction and initial labeling...\n",
      "Extracted 597659 sentences.\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting and initially labeling sentences from file...\")\n",
    "sentences_with_labels = extract_and_label_sentences(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aa7S0FRhoAj"
   },
   "source": [
    "Processing sentences based on labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1706554665791,
     "user": {
      "displayName": "Begüm Nur",
      "userId": "03114851068835061678"
     },
     "user_tz": -180
    },
    "id": "IUgm4Mlhhq_V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentences based on labels...\n",
      "Total processed sentences: 597659\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing sentences based on labels...\")\n",
    "processed_sentences = [merge_de_ki(sentence) if label == 1 else sentence\n",
    "                       for sentence, label in sentences_with_labels]\n",
    "print(f\"Total processed sentences: {len(processed_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1706554665792,
     "user": {
      "displayName": "Begüm Nur",
      "userId": "03114851068835061678"
     },
     "user_tz": -180
    },
    "id": "-D1u7CFThz43"
   },
   "outputs": [],
   "source": [
    "# Tokenizer with limited vocabulary size\n",
    "VOCAB_SIZE = 20000  # Adjust this to a smaller number if needed\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(processed_sentences)\n",
    "sequences = tokenizer.texts_to_sequences(processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1706554665792,
     "user": {
      "displayName": "Begüm Nur",
      "userId": "03114851068835061678"
     },
     "user_tz": -180
    },
    "id": "OfZ-C_ZUhz_F"
   },
   "outputs": [],
   "source": [
    "# Limit the maximum sequence length\n",
    "MAX_SEQUENCE_LENGTH = 100  # Adjust this based on the average length of your sentences\n",
    "padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1706554665792,
     "user": {
      "displayName": "Begüm Nur",
      "userId": "03114851068835061678"
     },
     "user_tz": -180
    },
    "id": "nQ7LIUCkh0CB"
   },
   "outputs": [],
   "source": [
    "# Prepare labels\n",
    "labels = [label for _, label in sentences_with_labels]\n",
    "\n",
    "# Combining sequences and labels for undersampling\n",
    "combined_data = list(zip(padded_sequences, labels))\n",
    "\n",
    "# Separating data by class for undersampling\n",
    "separated_data = [data for data in combined_data if data[1] == 1]\n",
    "non_separated_data = [data for data in combined_data if data[1] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1706554665792,
     "user": {
      "displayName": "Begüm Nur",
      "userId": "03114851068835061678"
     },
     "user_tz": -180
    },
    "id": "Wg2poNgQh0Et"
   },
   "outputs": [],
   "source": [
    "# Performing undersampling\n",
    "undersampled_non_separated = random.sample(non_separated_data, len(separated_data))\n",
    "balanced_data = separated_data + undersampled_non_separated\n",
    "random.shuffle(balanced_data)\n",
    "\n",
    "# Extracting balanced sequences and labels\n",
    "balanced_sequences = [data[0] for data in balanced_data]\n",
    "balanced_labels = [data[1] for data in balanced_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1706554665793,
     "user": {
      "displayName": "Begüm Nur",
      "userId": "03114851068835061678"
     },
     "user_tz": -180
    },
    "id": "PQLRYFWJh0G1"
   },
   "outputs": [],
   "source": [
    "# Use a smaller subset of data for training\n",
    "SAMPLE_SIZE = 100000  # Adjust this to your preference\n",
    "sample_indices = random.sample(range(len(balanced_sequences)), SAMPLE_SIZE)\n",
    "sample_sequences = [balanced_sequences[i] for i in sample_indices]\n",
    "sample_labels = [balanced_labels[i] for i in sample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1706554665793,
     "user": {
      "displayName": "Begüm Nur",
      "userId": "03114851068835061678"
     },
     "user_tz": -180
    },
    "id": "ZW5c6AcMiAVd"
   },
   "outputs": [],
   "source": [
    "# Splitting data into training and testing sets\n",
    "train_sequences_sample, test_sequences_sample, train_labels_sample, test_labels_sample = train_test_split(\n",
    "    sample_sequences, sample_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1706554665793,
     "user": {
      "displayName": "Begüm Nur",
      "userId": "03114851068835061678"
     },
     "user_tz": -180
    },
    "id": "Y7el-yrriAYB"
   },
   "outputs": [],
   "source": [
    "# Converting labels to numpy array\n",
    "train_labels_sample = np.array(train_labels_sample)\n",
    "test_labels_sample = np.array(test_labels_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1706554665793,
     "user": {
      "displayName": "Begüm Nur",
      "userId": "03114851068835061678"
     },
     "user_tz": -180
    },
    "id": "8kdiRMbKiAb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\BEGUM\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(VOCAB_SIZE, 32, input_length=MAX_SEQUENCE_LENGTH),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1706554665793,
     "user": {
      "displayName": "Begüm Nur",
      "userId": "03114851068835061678"
     },
     "user_tz": -180
    },
    "id": "1mjC0yo4iGi9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\BEGUM\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1706554665793,
     "user": {
      "displayName": "Begüm Nur",
      "userId": "03114851068835061678"
     },
     "user_tz": -180
    },
    "id": "HARh4EguiGlL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\BEGUM\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\BEGUM\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2000/2000 [==============================] - 50s 24ms/step - loss: 0.5360 - accuracy: 0.7110 - val_loss: 0.4684 - val_accuracy: 0.7604\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 52s 26ms/step - loss: 0.3994 - accuracy: 0.8091 - val_loss: 0.4908 - val_accuracy: 0.7531\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 55s 27ms/step - loss: 0.3051 - accuracy: 0.8590 - val_loss: 0.5628 - val_accuracy: 0.7405\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 55s 28ms/step - loss: 0.2056 - accuracy: 0.9086 - val_loss: 0.8700 - val_accuracy: 0.7323\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 54s 27ms/step - loss: 0.1307 - accuracy: 0.9440 - val_loss: 1.1369 - val_accuracy: 0.7305\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 54s 27ms/step - loss: 0.0871 - accuracy: 0.9647 - val_loss: 1.4884 - val_accuracy: 0.7272\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 55s 28ms/step - loss: 0.0571 - accuracy: 0.9781 - val_loss: 1.8331 - val_accuracy: 0.7221\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 56s 28ms/step - loss: 0.0391 - accuracy: 0.9845 - val_loss: 2.5251 - val_accuracy: 0.7244\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 58s 29ms/step - loss: 0.0310 - accuracy: 0.9888 - val_loss: 2.4814 - val_accuracy: 0.7164\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 52s 26ms/step - loss: 0.0252 - accuracy: 0.9905 - val_loss: 2.7412 - val_accuracy: 0.7209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1c058b1ecd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training with reduced epochs\n",
    "model.fit(np.array(train_sequences_sample), train_labels_sample, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1706554665793,
     "user": {
      "displayName": "Begüm Nur",
      "userId": "03114851068835061678"
     },
     "user_tz": -180
    },
    "id": "WZ4itzw3iGoH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 6s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "predicted_probabilities = model.predict(np.array(test_sequences_sample)).flatten()\n",
    "predicted_labels = [1 if prob > 0.5 else 0 for prob in predicted_probabilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1706554665794,
     "user": {
      "displayName": "Begüm Nur",
      "userId": "03114851068835061678"
     },
     "user_tz": -180
    },
    "id": "zHrXAGCMiLER"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation:\n",
      "Accuracy: 0.7212, Precision: 0.7196, Recall: 0.7196, F1-Score: 0.7196\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metrics\n",
    "accuracy = accuracy_score(test_labels_sample, predicted_labels)\n",
    "precision = precision_score(test_labels_sample, predicted_labels)\n",
    "recall = recall_score(test_labels_sample, predicted_labels)\n",
    "f1 = f1_score(test_labels_sample, predicted_labels)\n",
    "\n",
    "print(f\"Model Evaluation:\\nAccuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxvuQbZwk9_L"
   },
   "source": [
    "Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1706554665794,
     "user": {
      "displayName": "Begüm Nur",
      "userId": "03114851068835061678"
     },
     "user_tz": -180
    },
    "id": "Eob3s6Bpk_5o"
   },
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"Okuldan eve dönünce derslerinide yapacak.\",\n",
    "    \"Bu kitapları okuduktan sonra seninkileride alacağım.\",\n",
    "    \"Arkadaşlarıyla parkta oynadı ve yorulduklarında eve de döndüler.\",\n",
    "    \"Anneside bu durumdan memnun kalmıştı.\",\n",
    "    \"Oyun oynarken zamanın nasıl geçtiğinide anlamadılar.\",\n",
    "    \"Öğretmende sorunun cevabını bilmiyordu.\",\n",
    "    \"Kedileride yanlarına alarak yola çıktılar.\",\n",
    "    \"Sende mi geleceksin?\",\n",
    "    \"Bu konudaki görüşlerini merak ediyorum.\",\n",
    "    \"Dün gördüğümüz filmdeki karakter çok etkileyiciydi.\",\n",
    "    \"Karşıdaki evdeki ışıklar hâlâ yanıyor.\",\n",
    "    \"Bu konudaki düşüncelerini değiştirebilir miyim?\",\n",
    "    \"Toplantıdaki herkes fikrini açıkça belirtti.\",\n",
    "    \"Okuldaki sınavlarımız haftaya başlayacak.\",\n",
    "    \"Yoldaki engelleri aşarak hedefine ulaştı.\",\n",
    "    \"Dolaptaki elbiseleri değiştirmek istiyorum.\",\n",
    "    \"Bu akşamki planlarınızı iptal etmek zorunda kaldık.\",\n",
    "    \"Balkondaki çiçekler susuz kalmış.\",\n",
    "    \"Kütüphanedeki kitaplar çok eski.\",\n",
    "    \"Bahçedeki ağaçların altında piknik yapalım.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VqZUN8BlCR_"
   },
   "source": [
    "Results For Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1706554665794,
     "user": {
      "displayName": "Begüm Nur",
      "userId": "03114851068835061678"
     },
     "user_tz": -180
    },
    "id": "FnkRJhXOlD4U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 441ms/step\n",
      "Sentence: Okuldan eve dönünce derslerinide yapacak. - Not Separate\n",
      "Sentence: Bu kitapları okuduktan sonra seninkileride alacağım. - Not Separate\n",
      "Sentence: Arkadaşlarıyla parkta oynadı ve yorulduklarında eve de döndüler. - Separate\n",
      "Sentence: Anneside bu durumdan memnun kalmıştı. - Separate\n",
      "Sentence: Oyun oynarken zamanın nasıl geçtiğinide anlamadılar. - Not Separate\n",
      "Sentence: Öğretmende sorunun cevabını bilmiyordu. - Separate\n",
      "Sentence: Kedileride yanlarına alarak yola çıktılar. - Separate\n",
      "Sentence: Sende mi geleceksin? - Separate\n",
      "Sentence: Bu konudaki görüşlerini merak ediyorum. - Not Separate\n",
      "Sentence: Dün gördüğümüz filmdeki karakter çok etkileyiciydi. - Separate\n",
      "Sentence: Karşıdaki evdeki ışıklar hâlâ yanıyor. - Not Separate\n",
      "Sentence: Bu konudaki düşüncelerini değiştirebilir miyim? - Separate\n",
      "Sentence: Toplantıdaki herkes fikrini açıkça belirtti. - Not Separate\n",
      "Sentence: Okuldaki sınavlarımız haftaya başlayacak. - Not Separate\n",
      "Sentence: Yoldaki engelleri aşarak hedefine ulaştı. - Not Separate\n",
      "Sentence: Dolaptaki elbiseleri değiştirmek istiyorum. - Not Separate\n",
      "Sentence: Bu akşamki planlarınızı iptal etmek zorunda kaldık. - Separate\n",
      "Sentence: Balkondaki çiçekler susuz kalmış. - Separate\n",
      "Sentence: Kütüphanedeki kitaplar çok eski. - Not Separate\n",
      "Sentence: Bahçedeki ağaçların altında piknik yapalım. - Separate\n"
     ]
    }
   ],
   "source": [
    "# Process the test sentences as done with the training data\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "# Predict using the model\n",
    "test_predictions = model.predict(test_padded).flatten()\n",
    "\n",
    "# Interpret the predictions\n",
    "predicted_labels = [1 if prob > 0.5 else 0 for prob in test_predictions]\n",
    "\n",
    "# Display the results\n",
    "for sentence, label in zip(test_sentences, predicted_labels):\n",
    "    print(f\"Sentence: {sentence} - {'Separate' if label == 1 else 'Not Separate'}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOcykNc/ZAL3gseg6Sf19ZO",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
